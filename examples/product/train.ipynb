{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1f1a59-005a-419f-8452-b1bc98beae10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T23:23:00.853510Z",
     "iopub.status.busy": "2023-11-09T23:23:00.853071Z",
     "iopub.status.idle": "2023-11-09T23:23:00.874679Z",
     "shell.execute_reply": "2023-11-09T23:23:00.873922Z",
     "shell.execute_reply.started": "2023-11-09T23:23:00.853469Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c447e-83c2-4438-b3c1-d9e463d171ee",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T00:04:29.729932Z",
     "iopub.status.idle": "2023-11-10T00:04:29.730193Z",
     "shell.execute_reply": "2023-11-10T00:04:29.730083Z",
     "shell.execute_reply.started": "2023-11-10T00:04:29.730072Z"
    }
   },
   "outputs": [],
   "source": [
    "from rtb.datasets import get_dataset\n",
    "from rtb.utils import make_pkey_fkey_graph\n",
    "\n",
    "import torch\n",
    "import torch_frame as pyf\n",
    "import torch_geometric as pyg\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e204a4-5015-4133-861d-c2c4572d2798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T23:23:07.058748Z",
     "iopub.status.busy": "2023-11-09T23:23:07.058466Z",
     "iopub.status.idle": "2023-11-09T23:23:07.082372Z",
     "shell.execute_reply": "2023-11-09T23:23:07.081333Z",
     "shell.execute_reply.started": "2023-11-09T23:23:07.058729Z"
    }
   },
   "outputs": [],
   "source": [
    "# XXX: maybe we can abstract out a class for tabular encoder + GNN models\n",
    "# and put it under rtb.models\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, node_col_stats, node_col_names_dict, hetero_metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        # make node encoders (tabular encoders)\n",
    "        self.encs = torch.nn.ModuleDict()\n",
    "        for name in node_col_stats.keys():\n",
    "            self.encs[name] = pyf.nn.models.ResNet(\n",
    "                channels=64,\n",
    "                out_channels=64,\n",
    "                num_layers=4,\n",
    "                col_stats=node_col_stats[name],\n",
    "                col_names_dict=node_col_names_dict[name],\n",
    "            )\n",
    "\n",
    "        # make hetero GNN\n",
    "        self.gnn = pyg.nn.to_hetero(\n",
    "            pyg.nn.models.GCN(\n",
    "                in_channels=64,\n",
    "                hidden_channels=64,\n",
    "                num_layers=2,\n",
    "                out_channels=2,\n",
    "            ),\n",
    "            hetero_metadata(),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tf_dict: Dict[str, pyf.data.TensorFrame],\n",
    "        edge_index_dict: Dict[str, torch.Tensor],\n",
    "    ):\n",
    "        # encode node features from tensor frames\n",
    "        x_dict = {}\n",
    "        for name, tf in tf_dict.items():\n",
    "            x_dict[name] = self.encs[name](tf)\n",
    "\n",
    "        # run GNN\n",
    "        return self.gnn(x_dict, edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1260c113-bcf8-4fac-a2f5-dd1cd39a4556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T00:02:22.112915Z",
     "iopub.status.busy": "2023-11-10T00:02:22.112643Z",
     "iopub.status.idle": "2023-11-10T00:04:29.728772Z",
     "shell.execute_reply": "2023-11-10T00:04:29.727527Z",
     "shell.execute_reply.started": "2023-11-10T00:02:22.112896Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading table ../../data/rtb-product/processed/db/customer.parquet...\n",
      "done in 16.06 seconds.\n",
      "loading table ../../data/rtb-product/processed/db/product.parquet...\n",
      "done in 4.20 seconds.\n",
      "loading table ../../data/rtb-product/processed/db/review.parquet...\n",
      "done in 104.13 seconds.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Units 'M', 'Y' and 'y' do not represent unambiguous timedelta values and are not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dset \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrtb-product\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lfs/hyperturing1/0/ranjanr/.mambaforge/envs/rtb/lib/python3.10/site-packages/rtb/datasets/__init__.py:8\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(name, *args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function to get a dataset by name.\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrtb-product\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mProductDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown dataset name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/lfs/hyperturing1/0/ranjanr/.mambaforge/envs/rtb/lib/python3.10/site-packages/rtb/data/dataset.py:54\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db\u001b[38;5;241m.\u001b[39mget_time_range()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_cutoff_time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_cutoff_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_cutoff_times()\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lfs/hyperturing1/0/ranjanr/.mambaforge/envs/rtb/lib/python3.10/site-packages/rtb/datasets/product.py:92\u001b[0m, in \u001b[0;36mProductDataset.get_tasks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tasks\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Task]:\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of tasks defined on the dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mltv\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mLTV\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}\n",
      "File \u001b[0;32m/lfs/hyperturing1/0/ranjanr/.mambaforge/envs/rtb/lib/python3.10/site-packages/rtb/datasets/product.py:27\u001b[0m, in \u001b[0;36mLTV.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     25\u001b[0m         target_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mltv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m         task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mREGRESSION,\n\u001b[0;32m---> 27\u001b[0m         test_time_window_sizes\u001b[38;5;241m=\u001b[39m[\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1Y\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[1;32m     28\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmape\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     29\u001b[0m     )\n",
      "File \u001b[0;32mtimedeltas.pyx:1820\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timedeltas.Timedelta.__new__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtimedeltas.pyx:650\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timedeltas.parse_timedelta_string\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtimedeltas.pyx:698\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timedeltas.timedelta_from_spec\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Units 'M', 'Y' and 'y' do not represent unambiguous timedelta values and are not supported."
     ]
    }
   ],
   "source": [
    "dset = get_dataset(name=\"rtb-product\", root=\"../../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e0c96d6-7f93-41af-9b52-b356747e9915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T00:05:50.281331Z",
     "iopub.status.busy": "2023-11-10T00:05:50.280888Z",
     "iopub.status.idle": "2023-11-10T00:05:50.305617Z",
     "shell.execute_reply": "2023-11-10T00:05:50.305062Z",
     "shell.execute_reply.started": "2023-11-10T00:05:50.281295Z"
    }
   },
   "outputs": [],
   "source": [
    "from rtb.datasets.product import LTV\n",
    "\n",
    "dset.tasks[\"ltv\"] = LTV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecdbd5db-7a60-413f-9681-9ca91fccbd9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T00:05:52.119604Z",
     "iopub.status.busy": "2023-11-10T00:05:52.119260Z",
     "iopub.status.idle": "2023-11-10T00:05:52.149628Z",
     "shell.execute_reply": "2023-11-10T00:05:52.149000Z",
     "shell.execute_reply.started": "2023-11-10T00:05:52.119569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('7 days 00:00:00')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = dset.tasks[\"ltv\"].test_time_window_sizes[0]\n",
    "window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599f993-8f0e-44dd-a5c7-95906b1c4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important: node col stats should be computed only over the train set\n",
    "node_col_stats = {}\n",
    "node_col_names_dict = {}\n",
    "for name, table in dset.db_train.tables:\n",
    "    pyf_dataset = rtb.utils.to_pyf_dataset(table)\n",
    "    node_col_stats[name] = pyf_dataset.col_stats\n",
    "    # XXX: col_names_dict is not a pyf_dataset attribute, but maybe should be?\n",
    "    node_col_names_dict[name] = pyf_dataset.col_names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be85c9-ffb4-46db-b3c2-0785ec7bda0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make graph only for the train snapshot for safety\n",
    "data = make_pkey_fkey_graph(dset.db_train)\n",
    "net = Net(\n",
    "    node_col_stats=node_col_stats,\n",
    "    node_col_names_dict=node_col_names_dict,\n",
    "    hetero_metadata=data.metadata(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5251f-3d16-437d-8ea9-729805b76d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842be29a-642e-42a6-b17c-37025a0eab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders capture temporal sampling\n",
    "\n",
    "train_table = dset.make_train_table(\"ltv\", window_size)\n",
    "input_node_type = train_table.fkeys.values()[0]\n",
    "train_loader = pyg.nn.NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[10] * 2,\n",
    "    shuffle=True,\n",
    "    input_nodes=(\n",
    "        input_node_type,\n",
    "        torch.tensor(train_table.df[train_table.fkeys.keys()[0]]),\n",
    "    ),\n",
    "    input_time=torch.tensor(train_table.df[train_table.time_col]),\n",
    "    time_attr=\"time_stamp\",\n",
    "    transform=rtb.utils.AddTargetLabelTransform(train_table.df[train_table.target_col]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c904f-d1e9-4d76-99ae-bdbc54785dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_table = dset.make_val_table(\"ltv\", window_size)\n",
    "val_loader = pyg.nn.NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[-1] * 2,\n",
    "    shuffle=False,\n",
    "    input_nodes=(\n",
    "        input_node_type,\n",
    "        torch.tensor(val_table.df[val_table.fkeys.keys()[0]]),\n",
    "    ),\n",
    "    input_time=torch.tensor(val_table.df[val_table.time_col]),\n",
    "    time_attr=\"time_stamp\",\n",
    "    transform=rtb.utils.AddTargetLabelTransform(val_table.df[val_table.target_col]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1451c-ef17-4eb7-b1a7-f32aabf37995",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    # train\n",
    "    net.train()\n",
    "    for batch in train_loader:\n",
    "        batch_size = batch[input_node_type].batch_size\n",
    "\n",
    "        out = net(batch.tf_dict, batch.edge_index_dict)\n",
    "        yhat = out[input_node_type][:batch_size]\n",
    "        loss = F.cross_entropy(yhat, batch.y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # validate\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for batch in val_loader:\n",
    "            batch_size = batch[input_node_type].batch_size\n",
    "\n",
    "            out = net(batch.tf_dict, batch.edge_index_dict)\n",
    "            yhat = out[input_node_type][:batch_size]\n",
    "\n",
    "            total_examples += batch_size\n",
    "            total_correct += int((yhat.argmax(-1) == batch.y).sum())\n",
    "\n",
    "    print(f\"Epoch {epoch}: accuracy={total_correct / total_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14365e06-a036-44d0-8c8c-23f41c9d9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the user cannot query the final snapshot of the database directly\n",
    "# to prevent leakage of test information\n",
    "\n",
    "# instead, we provide a method to create a test table through the dataset\n",
    "# here the sampler is not for the user to choose\n",
    "# the time window is fixed to be [val_cutoff_time, val_cutoff_time + time_window]\n",
    "test_table = dset.make_test_table(\"ltv\", WEEK)\n",
    "\n",
    "# the input graph for the test is the snapshot of the database at val_cutoff_time\n",
    "data = rtb.utils.make_graph(db_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26a5ea-8cba-4710-b0d9-6a31c02e43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = pyg.nn.NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[-1] * 2,\n",
    "    shuffle=False,\n",
    "    input_nodes=(\n",
    "        input_node_type,\n",
    "        torch.tensor(test_table.df[test_table.fkeys.keys()[0]]),\n",
    "    ),\n",
    "    input_time=torch.tensor(test_table.df[test_table.time_col]),\n",
    "    time_attr=\"time_stamp\",\n",
    "    # note that AddTargetLabelTransform is not used here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f9463-838a-46e0-8352-c90d16e4da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "\n",
    "    yhats = []\n",
    "    for batch in test_loader:\n",
    "        batch_size = batch[input_node_type].batch_size\n",
    "\n",
    "        out = net(batch.tf_dict, batch.edge_index_dict)\n",
    "        yhat = out[input_node_type][:batch_size]\n",
    "\n",
    "        yhats.append(yhat)\n",
    "\n",
    "    yhat = torch.cat(yhats, dim=0)\n",
    "\n",
    "# the test ground-truth labels are also not exposed to the user\n",
    "print(dset.evaluate(\"ltv\", yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rtb]",
   "language": "python",
   "name": "conda-env-rtb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
